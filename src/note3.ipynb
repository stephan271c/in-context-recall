{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb95e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Iteration: 10, Outer Loss: 1.0416\n",
      "Outer Iteration: 20, Outer Loss: 1.0340\n",
      "Outer Iteration: 30, Outer Loss: 1.0262\n",
      "Outer Iteration: 40, Outer Loss: 1.0184\n",
      "Outer Iteration: 50, Outer Loss: 1.0107\n",
      "Outer Iteration: 60, Outer Loss: 1.0031\n",
      "Outer Iteration: 70, Outer Loss: 0.9955\n",
      "Outer Iteration: 80, Outer Loss: 0.9878\n",
      "Outer Iteration: 90, Outer Loss: 0.9799\n",
      "Outer Iteration: 100, Outer Loss: 0.9723\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchopt\n",
    "\n",
    "# Define the dimensions of the vectors\n",
    "key_dim = 10\n",
    "val_dim = 5\n",
    "hidden_dim = 20\n",
    "\n",
    "# Define the two neural networks, f and g\n",
    "class F_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(key_dim + hidden_dim, 8)\n",
    "        self.fc2 = nn.Linear(8, val_dim)\n",
    "\n",
    "    def forward(self, key, g_out):\n",
    "        x = torch.cat([key, g_out], dim=-1)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class G_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(key_dim, 8)\n",
    "        self.fc2 = nn.Linear(8, hidden_dim)\n",
    "\n",
    "    def forward(self, key):\n",
    "        x = F.sigmoid(self.fc1(key))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Instantiate the networks\n",
    "f_net = F_Net()\n",
    "g_net = G_Net()\n",
    "\n",
    "# Define the meta-optimizer for the inner loop (f_net)\n",
    "# We use MetaAdam, which allows us to differentiate through the optimization process.\n",
    "inner_optimizer = torchopt.MetaAdam(f_net, lr=1e-3)\n",
    "\n",
    "# Define the optimizer for the outer loop (g_net)\n",
    "outer_optimizer = torch.optim.Adam(g_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Define the loss functions\n",
    "inner_loss_fn = nn.MSELoss()\n",
    "# For the outer loss, we want to minimize the cosine distance, which is equivalent\n",
    "# to maximizing the cosine similarity. We can achieve this by taking 1 minus the\n",
    "# cosine similarity.\n",
    "outer_loss_fn = lambda pred, target: 1 - F.cosine_similarity(pred, target).mean()\n",
    "\n",
    "# --- Training Loop ---\n",
    "\n",
    "# Generate a sequence of dummy data\n",
    "num_steps = 5\n",
    "batch_size = 4\n",
    "sequence_of_keys = [torch.randn(batch_size, key_dim) for _ in range(num_steps)]\n",
    "sequence_of_vals = [torch.randn(batch_size, val_dim) for _ in range(num_steps)]\n",
    "\n",
    "# We need to save the initial state of f_net to reset it for each outer loop iteration\n",
    "f_net_initial_state = torchopt.extract_state_dict(f_net)\n",
    "inner_optim_initial_state = torchopt.extract_state_dict(inner_optimizer)\n",
    "\n",
    "# Number of outer loop iterations\n",
    "for outer_iter in range(100):\n",
    "    # Reset f_net to its initial state before each outer loop\n",
    "    torchopt.recover_state_dict(f_net, f_net_initial_state)\n",
    "    torchopt.recover_state_dict(inner_optimizer, inner_optim_initial_state)\n",
    "\n",
    "    outer_optimizer.zero_grad()\n",
    "\n",
    "    total_outer_loss = 0.0\n",
    "\n",
    "    # --- Inner Loop ---\n",
    "    # Traverse the sequence of key-value pairs\n",
    "    for i in range(num_steps):\n",
    "        key = sequence_of_keys[i]\n",
    "        val = sequence_of_vals[i]\n",
    "\n",
    "        # Get the output of g(key)\n",
    "        g_output = g_net(key)\n",
    "\n",
    "        # Get the prediction from f(key, g(key))\n",
    "        f_output = f_net(key, g_output)\n",
    "\n",
    "        # Calculate the inner loss\n",
    "        inner_loss = inner_loss_fn(f_output, val)\n",
    "\n",
    "        # Update f_net using the meta-optimizer\n",
    "        # This step is differentiable\n",
    "        inner_optimizer.step(inner_loss)\n",
    "\n",
    "    # --- Outer Loop Loss Calculation ---\n",
    "    # After the inner loop, calculate the outer loss on the last key-value pair\n",
    "    # using the updated f_net\n",
    "    final_key = sequence_of_keys[-1]\n",
    "    final_val = sequence_of_vals[-1]\n",
    "    final_g_output = g_net(final_key)\n",
    "    final_f_output = f_net(final_key, final_g_output)\n",
    "\n",
    "    outer_loss = outer_loss_fn(final_f_output, final_val)\n",
    "\n",
    "    # Backpropagate the outer loss through the unrolled inner loop\n",
    "    # This will compute gradients for g_net's parameters\n",
    "    outer_loss.backward()\n",
    "\n",
    "    # Update g_net's parameters\n",
    "    outer_optimizer.step()\n",
    "\n",
    "    if (outer_iter + 1) % 10 == 0:\n",
    "\n",
    "        print(f\"Outer Iteration: {outer_iter + 1}, Outer Loss: {outer_loss.item():.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
