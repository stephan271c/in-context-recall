{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888276a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training on cuda ---\n",
      "Epoch 10 | Avg Outer Loss: 3.9763\n",
      "  Sample Hyperparams -> LR: 0.1244\n",
      "Epoch 50 | Avg Outer Loss: 3.9326\n",
      "  Sample Hyperparams -> LR: 0.1000\n",
      "Epoch 100 | Avg Outer Loss: 3.8691\n",
      "  Sample Hyperparams -> LR: 0.1307\n",
      "Epoch 150 | Avg Outer Loss: 3.9136\n",
      "  Sample Hyperparams -> LR: 0.1088\n",
      "Epoch 200 | Avg Outer Loss: 3.9146\n",
      "  Sample Hyperparams -> LR: 0.1210\n",
      "Epoch 250 | Avg Outer Loss: 3.8483\n",
      "  Sample Hyperparams -> LR: 0.0931\n",
      "Epoch 300 | Avg Outer Loss: 3.8917\n",
      "  Sample Hyperparams -> LR: 0.1351\n",
      "Epoch 350 | Avg Outer Loss: 3.8661\n",
      "  Sample Hyperparams -> LR: 0.0791\n",
      "Epoch 400 | Avg Outer Loss: 3.8964\n",
      "  Sample Hyperparams -> LR: 0.1105\n",
      "Epoch 450 | Avg Outer Loss: 3.8560\n",
      "  Sample Hyperparams -> LR: 0.0743\n",
      "Epoch 500 | Avg Outer Loss: 3.8786\n",
      "  Sample Hyperparams -> LR: 0.0869\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import numpy as np\n",
    "from torch.func import functional_call\n",
    "\n",
    "from func_memory_module import TTTMLP, WeightModel, HyperparamModel\n",
    "from synthetic_datasets import InContextRecallDataset\n",
    "from meta_optimizers import ManualAdam\n",
    "from losses import windowed_p_loss, windowed_recall_cross_entropy\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "device_type = \"cuda\"  # Set to \"cuda\" for GPU or \"cpu\" to force CPU\n",
    "if device_type == \"cuda\" and not torch.cuda.is_available():\n",
    "    print(\"CUDA not available; falling back to CPU.\")\n",
    "    device_type = \"cpu\"\n",
    "device = torch.device(device_type)\n",
    "\n",
    "key_dim = 16\n",
    "val_dim = 16\n",
    "context_dim = 5  # Placeholder loss takes a single weight\n",
    "seq_len = 50\n",
    "num_epochs = 500  # Total number of task sequences to process\n",
    "batch_size = 10\n",
    "assert num_epochs % batch_size == 0, \"batch_size must divide num_epochs\"\n",
    "num_meta_updates = num_epochs // batch_size\n",
    "recall_window = 1  # Number of timesteps to score in the outer loss window\n",
    "\n",
    "# --- Instantiate all meta-learning models ---\n",
    "weight_model = WeightModel(key_dim, context_dim).to(device)\n",
    "memory_module = TTTMLP(key_dim, val_dim).to(device)\n",
    "lr_model = HyperparamModel(key_dim, initial_bias=-2.0).to(device)  # Start with LR ~0.12\n",
    "\n",
    "# --- Instantiate optimizers ---\n",
    "# The outer-loop optimizer that trains ALL meta-models\n",
    "outer_optimizer = torch.optim.AdamW(\n",
    "    list(memory_module.parameters()) +\n",
    "    list(weight_model.parameters()) +\n",
    "    list(lr_model.parameters()),\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# --- Loss Functions ---\n",
    "inner_loss_func = windowed_p_loss\n",
    "outer_loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"--- Starting Training on {device} ---\")\n",
    "for meta_step in range(num_meta_updates):\n",
    "    # Generate a batch of new tasks (sequences) for this meta-update\n",
    "    batch_datasets = []\n",
    "    for _ in range(batch_size):\n",
    "        dataset = InContextRecallDataset(\n",
    "            seq_len=seq_len,\n",
    "            key_dim=key_dim,\n",
    "            val_dim=val_dim,\n",
    "            context_size=context_dim,\n",
    "            output_corr=0.5,\n",
    "        )\n",
    "        batch_datasets.append((dataset, dataset.inputs.to(device), dataset.targets.to(device)))\n",
    "\n",
    "    # Reset inner models and optimizer states for every sequence in the batch\n",
    "    fast_model = copy.deepcopy(memory_module).to(device)\n",
    "    inner_optimizer = ManualAdam()\n",
    "\n",
    "    param_sets = []\n",
    "    state_list = []\n",
    "    for _ in range(batch_size):\n",
    "        params = {}\n",
    "        for name, p in fast_model.named_parameters():\n",
    "            cloned = p.detach().clone().to(device)\n",
    "            cloned.requires_grad_(True)\n",
    "            params[name] = cloned\n",
    "        param_sets.append(params)\n",
    "        state_list.append(inner_optimizer.init_states(params))\n",
    "\n",
    "    outer_optimizer.zero_grad()\n",
    "    total_outer_loss = torch.zeros((), device=device)\n",
    "\n",
    "    # --- Inner Loop ---\n",
    "    for time_index in range(seq_len):\n",
    "        for task_idx, (dataset, inputs_device, targets_device) in enumerate(batch_datasets):\n",
    "            current_key, current_val = dataset[time_index]\n",
    "            current_key = current_key.to(device)\n",
    "            current_val = current_val.to(device)\n",
    "\n",
    "            # 1. Get dynamic parameters from meta-models\n",
    "            loss_weights = weight_model(current_key[-1])  # only current key\n",
    "            hyperparams = {\n",
    "                \"lr\": lr_model(current_key[-1]),\n",
    "                \"beta1\": 0.95,\n",
    "                \"beta2\": 0.99,\n",
    "            }\n",
    "\n",
    "            params = param_sets[task_idx]\n",
    "            state = state_list[task_idx]\n",
    "\n",
    "            # 2. Calculate inner loss and gradients using the fast_model\n",
    "            predictions = functional_call(fast_model, params, current_key)\n",
    "            inner_loss = inner_loss_func(predictions.T, current_val.T, loss_weights)\n",
    "            grad_tuple = torch.autograd.grad(inner_loss, tuple(params.values()), create_graph=True)\n",
    "            grads = dict(zip(params.keys(), grad_tuple))\n",
    "\n",
    "            # 3. Step the inner optimizer for the fast_model\n",
    "            updated_params, updated_state = inner_optimizer.step(params, grads, state, **hyperparams)\n",
    "            param_sets[task_idx] = updated_params\n",
    "            state_list[task_idx] = updated_state\n",
    "\n",
    "            # 4. Calculate outer loss using the updated fast_model\n",
    "            outer_loss_step = windowed_recall_cross_entropy(\n",
    "                fast_model,\n",
    "                updated_params,\n",
    "                inputs_device,\n",
    "                targets_device,\n",
    "                time_index=time_index,\n",
    "                window_size=recall_window,\n",
    "                loss_fn=outer_loss_func,\n",
    "            )\n",
    "            total_outer_loss = total_outer_loss + outer_loss_step\n",
    "\n",
    "    # --- Outer Loop Update (Meta-Model Training) ---\n",
    "    final_outer_loss = total_outer_loss / (seq_len * batch_size)\n",
    "    final_outer_loss.backward()\n",
    "    outer_optimizer.step()\n",
    "\n",
    "    processed_sequences = (meta_step + 1) * batch_size\n",
    "    if meta_step == 0 or processed_sequences % 50 == 0:\n",
    "        print(f\"Epoch {processed_sequences} | Avg Outer Loss: {final_outer_loss.item():.4f}\")\n",
    "        sample_key = batch_datasets[0][1][0].unsqueeze(0)\n",
    "        lr_sample = lr_model(sample_key).item()\n",
    "        print(f\"  Sample Hyperparams -> LR: {lr_sample:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
