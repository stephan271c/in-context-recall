{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7305ccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, Loss Weights: tensor([0.5236, 0.4659, 0.5576, 0.4184, 0.4073], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 0, Step 3, Loss Weights: tensor([0.6497, 0.4965, 0.5808, 0.4881, 0.4623], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 0, Step 6, Loss Weights: tensor([0.5867, 0.3844, 0.4811, 0.5296, 0.4653], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 0, Step 9, Loss Weights: tensor([0.4678, 0.5109, 0.5481, 0.5413, 0.4111], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 0, Step 12, Loss Weights: tensor([0.4525, 0.4480, 0.4397, 0.4338, 0.4029], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 0, Step 15, Loss Weights: tensor([0.3771, 0.3392, 0.3810, 0.6110, 0.6096], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 0, Step 18, Loss Weights: tensor([0.6107, 0.6313, 0.5277, 0.4985, 0.4253], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 0, Outer Loss: 2.6988954544067383\n",
      "Epoch 10, Step 0, Loss Weights: tensor([0.4521, 0.5026, 0.3426, 0.6472, 0.7512], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 10, Step 3, Loss Weights: tensor([0.3757, 0.3497, 0.2867, 0.4909, 0.5674], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 10, Step 6, Loss Weights: tensor([0.4823, 0.2700, 0.3224, 0.5372, 0.6122], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 10, Step 9, Loss Weights: tensor([0.4303, 0.5195, 0.4464, 0.6039, 0.7208], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 10, Step 12, Loss Weights: tensor([0.4199, 0.6035, 0.4776, 0.5932, 0.6614], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 10, Step 15, Loss Weights: tensor([0.2302, 0.4223, 0.4403, 0.7579, 0.5999], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 10, Step 18, Loss Weights: tensor([0.2837, 0.6056, 0.4578, 0.7249, 0.5989], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 10, Outer Loss: 2.618020534515381\n",
      "Epoch 20, Step 0, Loss Weights: tensor([0.2771, 0.2567, 0.1894, 0.7576, 0.7915], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 20, Step 3, Loss Weights: tensor([0.3104, 0.1755, 0.2545, 0.6708, 0.7812], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 20, Step 6, Loss Weights: tensor([0.2298, 0.2848, 0.3756, 0.7858, 0.7628], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 20, Step 9, Loss Weights: tensor([0.2991, 0.2631, 0.2505, 0.7070, 0.6812], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 20, Step 12, Loss Weights: tensor([0.1663, 0.3965, 0.4161, 0.8208, 0.7539], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 20, Step 15, Loss Weights: tensor([0.2307, 0.2611, 0.2407, 0.7360, 0.6547], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 20, Step 18, Loss Weights: tensor([0.3595, 0.2715, 0.4828, 0.6649, 0.7240], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 20, Outer Loss: 2.530492067337036\n",
      "Epoch 30, Step 0, Loss Weights: tensor([0.2972, 0.1344, 0.3106, 0.7307, 0.7669], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 30, Step 3, Loss Weights: tensor([0.1603, 0.2740, 0.1940, 0.7746, 0.8422], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 30, Step 6, Loss Weights: tensor([0.2820, 0.1165, 0.2672, 0.7212, 0.7596], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 30, Step 9, Loss Weights: tensor([0.1384, 0.2334, 0.2122, 0.7600, 0.8057], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 30, Step 12, Loss Weights: tensor([0.1883, 0.4345, 0.2182, 0.7513, 0.8078], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 30, Step 15, Loss Weights: tensor([0.2435, 0.1579, 0.2086, 0.8222, 0.8327], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 30, Step 18, Loss Weights: tensor([0.2070, 0.0913, 0.1178, 0.7759, 0.8022], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 30, Outer Loss: 2.4854748249053955\n",
      "Epoch 40, Step 0, Loss Weights: tensor([0.1638, 0.2455, 0.1543, 0.8679, 0.9011], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 40, Step 3, Loss Weights: tensor([0.1661, 0.1019, 0.1246, 0.8692, 0.8960], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 40, Step 6, Loss Weights: tensor([0.1236, 0.1411, 0.1203, 0.8377, 0.8749], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 40, Step 9, Loss Weights: tensor([0.1518, 0.0966, 0.1958, 0.8653, 0.8478], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 40, Step 12, Loss Weights: tensor([0.1151, 0.0947, 0.1178, 0.8691, 0.8554], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 40, Step 15, Loss Weights: tensor([0.1519, 0.0576, 0.1220, 0.7632, 0.8247], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 40, Step 18, Loss Weights: tensor([0.0961, 0.1778, 0.1311, 0.8228, 0.8438], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 40, Outer Loss: 2.4739432334899902\n",
      "Epoch 50, Step 0, Loss Weights: tensor([0.1381, 0.0793, 0.0892, 0.8595, 0.8909], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 50, Step 3, Loss Weights: tensor([0.1373, 0.1749, 0.1415, 0.8501, 0.9015], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 50, Step 6, Loss Weights: tensor([0.1315, 0.1069, 0.1740, 0.8985, 0.8907], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 50, Step 9, Loss Weights: tensor([0.1059, 0.0581, 0.0913, 0.8207, 0.8790], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 50, Step 12, Loss Weights: tensor([0.1041, 0.1482, 0.1334, 0.8445, 0.8860], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 50, Step 15, Loss Weights: tensor([0.0974, 0.1163, 0.0868, 0.9008, 0.9106], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 50, Step 18, Loss Weights: tensor([0.0824, 0.1393, 0.0995, 0.9078, 0.9054], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 50, Outer Loss: 2.3900837898254395\n",
      "Epoch 60, Step 0, Loss Weights: tensor([0.0731, 0.0635, 0.0935, 0.9127, 0.9125], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 60, Step 3, Loss Weights: tensor([0.0530, 0.0418, 0.0518, 0.9053, 0.9146], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 60, Step 6, Loss Weights: tensor([0.0659, 0.0947, 0.0581, 0.8743, 0.9205], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 60, Step 9, Loss Weights: tensor([0.0822, 0.0698, 0.1172, 0.8859, 0.9128], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 60, Step 12, Loss Weights: tensor([0.0568, 0.0552, 0.0772, 0.9153, 0.9031], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 60, Step 15, Loss Weights: tensor([0.0715, 0.0610, 0.0638, 0.8680, 0.9129], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 60, Step 18, Loss Weights: tensor([0.0806, 0.0538, 0.0800, 0.8891, 0.9037], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 60, Outer Loss: 2.387251853942871\n",
      "Epoch 70, Step 0, Loss Weights: tensor([0.0714, 0.0345, 0.0389, 0.9116, 0.9359], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 70, Step 3, Loss Weights: tensor([0.0700, 0.0435, 0.0452, 0.8898, 0.9357], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 70, Step 6, Loss Weights: tensor([0.0636, 0.0634, 0.0424, 0.9108, 0.9395], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 70, Step 9, Loss Weights: tensor([0.0454, 0.0477, 0.0447, 0.9211, 0.9420], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 70, Step 12, Loss Weights: tensor([0.0540, 0.0385, 0.0303, 0.9028, 0.9392], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 70, Step 15, Loss Weights: tensor([0.0685, 0.0584, 0.0591, 0.9302, 0.9309], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 70, Step 18, Loss Weights: tensor([0.0533, 0.0443, 0.0688, 0.9186, 0.9265], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 70, Outer Loss: 2.4367830753326416\n",
      "Epoch 80, Step 0, Loss Weights: tensor([0.0377, 0.0457, 0.0412, 0.9376, 0.9422], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 80, Step 3, Loss Weights: tensor([0.0347, 0.0335, 0.0448, 0.9351, 0.9431], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 80, Step 6, Loss Weights: tensor([0.0491, 0.0439, 0.0431, 0.9164, 0.9404], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 80, Step 9, Loss Weights: tensor([0.0632, 0.0381, 0.0467, 0.9285, 0.9400], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 80, Step 12, Loss Weights: tensor([0.0598, 0.0653, 0.0577, 0.9123, 0.9403], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 80, Step 15, Loss Weights: tensor([0.0502, 0.0286, 0.0529, 0.9182, 0.9412], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 80, Step 18, Loss Weights: tensor([0.0532, 0.0476, 0.0397, 0.9238, 0.9467], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 80, Outer Loss: 2.3893942832946777\n",
      "Epoch 90, Step 0, Loss Weights: tensor([0.0346, 0.0398, 0.0371, 0.9288, 0.9504], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 90, Step 3, Loss Weights: tensor([0.0462, 0.0366, 0.0236, 0.9378, 0.9536], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 90, Step 6, Loss Weights: tensor([0.0397, 0.0404, 0.0494, 0.9239, 0.9454], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 90, Step 9, Loss Weights: tensor([0.0263, 0.0214, 0.0327, 0.9344, 0.9545], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 90, Step 12, Loss Weights: tensor([0.0575, 0.0436, 0.0465, 0.9231, 0.9459], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 90, Step 15, Loss Weights: tensor([0.0569, 0.0358, 0.0399, 0.9198, 0.9455], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 90, Step 18, Loss Weights: tensor([0.0367, 0.0443, 0.0341, 0.9400, 0.9497], grad_fn=<SigmoidBackward0>)\n",
      "Epoch 90, Outer Loss: 2.4771888256073\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchopt\n",
    "\n",
    "from losses import windowed_p_loss\n",
    "from func_memory_module import LinearModel, TTTMLP, WeightModel\n",
    "from synthetic_datasets import InContextRecallDataset\n",
    "\n",
    "\n",
    "# Define the dimensions of the vectors\n",
    "key_dim = 10\n",
    "val_dim = 10\n",
    "hidden_dim = 20\n",
    "context_dim = 5\n",
    "\n",
    "# instantiate models\n",
    "weight_model = WeightModel(key_dim, context_dim)\n",
    "memory_module= TTTMLP(key_dim, val_dim)\n",
    "\n",
    "# instantiate optimizers\n",
    "inner_optimizer = torchopt.MetaSGD(memory_module, lr=0.3)\n",
    "outer_optimizer = torch.optim.AdamW(weight_model.parameters(), lr=0.05)\n",
    "\n",
    "#define loss function\n",
    "inner_loss_func = windowed_p_loss\n",
    "outer_loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# training loop\n",
    "seq_len = 20\n",
    "\n",
    "# save initial state\n",
    "memory_module_init = torchopt.extract_state_dict(memory_module)\n",
    "inner_optimizer_init = torchopt.extract_state_dict(inner_optimizer)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Generate a new sequence of keys and values for each epoch\n",
    "    my_dataset = InContextRecallDataset(seq_len=seq_len, key_dim=key_dim, val_dim=val_dim, context_size=context_dim, output_corr=0.5)\n",
    "\n",
    "    _, val_vectors = my_dataset[:]\n",
    "\n",
    "    # Reset model and optimizer to initial state\n",
    "    torchopt.recover_state_dict(memory_module, memory_module_init)\n",
    "    torchopt.recover_state_dict(inner_optimizer, inner_optimizer_init)\n",
    "\n",
    "    outer_optimizer.zero_grad()\n",
    "    total_outer_loss = torch.tensor(0.0)\n",
    "\n",
    "    # inner loop\n",
    "    for i in range(seq_len):\n",
    "        input, output = my_dataset[i]\n",
    "        loss_weights = weight_model(input[-1]) # should only rely on current key in context\n",
    "        if epoch % 10 == 0 and i % 3 == 0:\n",
    "            print(f\"Epoch {epoch}, Step {i}, Loss Weights: {loss_weights}\")\n",
    "        pred = memory_module(input)\n",
    "\n",
    "        inner_loss = inner_loss_func(pred.T, output.T, loss_weights) # the loss has a transposed shape\n",
    "        inner_optimizer.step(inner_loss)\n",
    "\n",
    "        pred_after_update = memory_module(input[-1]) #now shape (1, val_dim) (we only take the last one.)\n",
    "        #print(pred_after_update.shape, val_vectors.T.shape)\n",
    "        logits = torch.matmul(pred_after_update, val_vectors.T) # shape (1, seq_len)\n",
    "        target_index = torch.tensor([i], dtype=torch.long) # shape (1,)\n",
    "        #print(temp.shape , target_index.shape, logits.shape)\n",
    "        #print(temp.dtype , target_index.dtype)\n",
    "        outer_loss_step = outer_loss_func(logits, target_index)\n",
    "        total_outer_loss += outer_loss_step\n",
    "    \n",
    "    # outer loop calculation\n",
    "    final_outer_loss = total_outer_loss / seq_len\n",
    "    final_outer_loss.backward()\n",
    "    outer_optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Outer Loss: {final_outer_loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
