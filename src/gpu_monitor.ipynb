{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Memory Utilization Dashboard\n",
    "\n",
    "This notebook provides a local equivalent to Google Colab's GPU memory dashboard. It monitors NVIDIA GPU memory usage, utilization, and temperature in real-time.\n",
    "\n",
    "Based on the `nvidia-ml-py` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPU(s)\n",
      "PyTorch CUDA devices: 1\n",
      "  Device 0: NVIDIA GeForce RTX 5070\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style\n",
    "import time\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import torch\n",
    "\n",
    "# Initialize NVIDIA management library\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "# Get number of GPUs\n",
    "device_count = pynvml.nvmlDeviceGetCount()\n",
    "print(f\"Found {device_count} GPU(s)\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"PyTorch CUDA devices: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA not available in PyTorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 5070\n",
      "Memory: 705.3MB / 12227.0MB (5.8%)\n",
      "GPU Utilization: 0%\n",
      "Temperature: 39°C\n"
     ]
    }
   ],
   "source": [
    "def get_gpu_info(device_id=0):\n",
    "    \"\"\"Get GPU memory and utilization information\"\"\"\n",
    "    try:\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)\n",
    "        \n",
    "        # Memory info\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        \n",
    "        # Utilization\n",
    "        util_info = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "        \n",
    "        # Temperature\n",
    "        try:\n",
    "            temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "        except:\n",
    "            temp = None\n",
    "        \n",
    "        # Device name\n",
    "        name = pynvml.nvmlDeviceGetName(handle)\n",
    "        \n",
    "        return {\n",
    "            'name': name.decode('utf-8') if isinstance(name, bytes) else name,\n",
    "            'memory_used': mem_info.used / 1024**2,  # MB\n",
    "            'memory_total': mem_info.total / 1024**2,  # MB\n",
    "            'memory_free': mem_info.free / 1024**2,    # MB\n",
    "            'memory_percent': (mem_info.used / mem_info.total) * 100,\n",
    "            'gpu_utilization': util_info.gpu,\n",
    "            'memory_utilization': util_info.memory,\n",
    "            'temperature': temp,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test the function\n",
    "gpu_info = get_gpu_info()\n",
    "if 'error' not in gpu_info:\n",
    "    print(f\"GPU: {gpu_info['name']}\")\n",
    "    print(f\"Memory: {gpu_info['memory_used']:.1f}MB / {gpu_info['memory_total']:.1f}MB ({gpu_info['memory_percent']:.1f}%)\")\n",
    "    print(f\"GPU Utilization: {gpu_info['gpu_utilization']}%\")\n",
    "    if gpu_info['temperature']:\n",
    "        print(f\"Temperature: {gpu_info['temperature']}°C\")\n",
    "else:\n",
    "    print(f\"Error: {gpu_info['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMonitor:\n",
    "    \"\"\"GPU monitoring class with data collection and plotting\"\"\"\n",
    "    \n",
    "    def __init__(self, device_id=0, max_history=100):\n",
    "        self.device_id = device_id\n",
    "        self.max_history = max_history\n",
    "        self.is_monitoring = False\n",
    "        self.monitor_thread = None\n",
    "        \n",
    "        # Data storage\n",
    "        self.timestamps = []\n",
    "        self.memory_used = []\n",
    "        self.memory_percent = []\n",
    "        self.gpu_utilization = []\n",
    "        self.temperatures = []\n",
    "    \n",
    "    def start_monitoring(self, interval=1.0):\n",
    "        \"\"\"Start background monitoring\"\"\"\n",
    "        if self.is_monitoring:\n",
    "            print(\"Already monitoring\")\n",
    "            return\n",
    "        \n",
    "        self.is_monitoring = True\n",
    "        self.monitor_thread = threading.Thread(target=self._monitor_loop, args=(interval,))\n",
    "        self.monitor_thread.daemon = True\n",
    "        self.monitor_thread.start()\n",
    "        print(\"GPU monitoring started\")\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop background monitoring\"\"\"\n",
    "        self.is_monitoring = False\n",
    "        if self.monitor_thread:\n",
    "            self.monitor_thread.join(timeout=2)\n",
    "        print(\"GPU monitoring stopped\")\n",
    "    \n",
    "    def _monitor_loop(self, interval):\n",
    "        \"\"\"Background monitoring loop\"\"\"\n",
    "        while self.is_monitoring:\n",
    "            info = get_gpu_info(self.device_id)\n",
    "            \n",
    "            if 'error' not in info:\n",
    "                self.timestamps.append(info['timestamp'])\n",
    "                self.memory_used.append(info['memory_used'])\n",
    "                self.memory_percent.append(info['memory_percent'])\n",
    "                self.gpu_utilization.append(info['gpu_utilization'])\n",
    "                \n",
    "                if info['temperature'] is not None:\n",
    "                    self.temperatures.append(info['temperature'])\n",
    "                \n",
    "                # Limit history\n",
    "                if len(self.timestamps) > self.max_history:\n",
    "                    self.timestamps.pop(0)\n",
    "                    self.memory_used.pop(0)\n",
    "                    self.memory_percent.pop(0)\n",
    "                    self.gpu_utilization.pop(0)\n",
    "                    if self.temperatures:\n",
    "                        self.temperatures.pop(0)\n",
    "            \n",
    "            time.sleep(interval)\n",
    "    \n",
    "    def get_current_stats(self):\n",
    "        \"\"\"Get current GPU statistics\"\"\"\n",
    "        info = get_gpu_info(self.device_id)\n",
    "        if 'error' not in info:\n",
    "            return info\n",
    "        return None\n",
    "    \n",
    "    def get_memory_usage_mb(self):\n",
    "        \"\"\"Get current memory usage in MB\"\"\"\n",
    "        stats = self.get_current_stats()\n",
    "        if stats:\n",
    "            return stats['memory_used'], stats['memory_total']\n",
    "        return 0, 0\n",
    "    \n",
    "    def get_memory_usage_percent(self):\n",
    "        \"\"\"Get current memory usage percentage\"\"\"\n",
    "        stats = self.get_current_stats()\n",
    "        if stats:\n",
    "            return stats['memory_percent']\n",
    "        return 0\n",
    "\n",
    "# Create monitor instance\n",
    "monitor = GPUMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7df626890e04b3fbcaa390eef3bda88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='GPU: NVIDIA GeForce RTX 5070\\nMemory: 705.3MB / 12227.0MB\\nGPU Util: 0%', descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create interactive dashboard widgets\n",
    "\n",
    "def create_dashboard():\n",
    "    \"\"\"Create an interactive GPU dashboard with widgets\"\"\"\n",
    "    \n",
    "    # Create widgets\n",
    "    status_text = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder='GPU status will appear here',\n",
    "        description='GPU Status:',\n",
    "        disabled=True,\n",
    "        layout=widgets.Layout(width='100%', height='120px')\n",
    "    )\n",
    "    \n",
    "    memory_progress = widgets.FloatProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=100,\n",
    "        description='Memory:',\n",
    "        bar_style='info',\n",
    "        style={'bar_color': '#1f77b4'}\n",
    "    )\n",
    "    \n",
    "    gpu_progress = widgets.FloatProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=100,\n",
    "        description='GPU Util:',\n",
    "        bar_style='success',\n",
    "        style={'bar_color': '#ff7f0e'}\n",
    "    )\n",
    "    \n",
    "    temp_display = widgets.Label(value='Temperature: N/A')\n",
    "    \n",
    "    start_button = widgets.Button(description='Start Monitoring')\n",
    "    stop_button = widgets.Button(description='Stop Monitoring')\n",
    "    \n",
    "    # Button callbacks\n",
    "    def on_start_clicked(b):\n",
    "        monitor.start_monitoring(interval=1.0)\n",
    "        update_dashboard()\n",
    "    \n",
    "    def on_stop_clicked(b):\n",
    "        monitor.stop_monitoring()\n",
    "    \n",
    "    start_button.on_click(on_start_clicked)\n",
    "    stop_button.on_click(on_stop_clicked)\n",
    "    \n",
    "    def update_dashboard():\n",
    "        \"\"\"Update dashboard with current GPU info\"\"\"\n",
    "        if monitor.is_monitoring:\n",
    "            start_button.disabled = True\n",
    "            stop_button.disabled = False\n",
    "        else:\n",
    "            start_button.disabled = False\n",
    "            stop_button.disabled = True\n",
    "        \n",
    "        stats = monitor.get_current_stats()\n",
    "        if stats:\n",
    "            used, total = stats['memory_used'], stats['memory_total']\n",
    "            status_text.value = f\"GPU: {stats['name']}\\n\" \\\n",
    "                               f\"Memory: {used:.1f}MB / {total:.1f}MB\\n\" \\\n",
    "                               f\"GPU Util: {stats['gpu_utilization']}%\"\n",
    "            \n",
    "            memory_progress.value = stats['memory_percent']\n",
    "            memory_progress.description = '.1f'\n",
    "            \n",
    "            gpu_progress.value = stats['gpu_utilization']\n",
    "            gpu_progress.description = f'GPU Util: {stats['gpu_utilization']}'\n",
    "            \n",
    "            if stats['temperature'] is not None:\n",
    "                temp_display.value = f'Temperature: {stats['temperature']}°C'\n",
    "        else:\n",
    "            status_text.value = \"Unable to get GPU information\"\n",
    "    \n",
    "    # Periodic update using threading\n",
    "    import threading\n",
    "    \n",
    "    def periodic_update():\n",
    "        while True:\n",
    "            if monitor.is_monitoring:\n",
    "                update_dashboard()\n",
    "            time.sleep(2)  # Update every 2 seconds\n",
    "    \n",
    "    update_thread = threading.Thread(target=periodic_update, daemon=True)\n",
    "    update_thread.start()\n",
    "    \n",
    "    # Initial update\n",
    "    update_dashboard()\n",
    "    \n",
    "    # Display widgets\n",
    "    controls = widgets.HBox([start_button, stop_button])\n",
    "    progress_bars = widgets.VBox([memory_progress, gpu_progress])\n",
    "    \n",
    "    return widgets.VBox([\n",
    "        status_text,\n",
    "        progress_bars,\n",
    "        temp_display,\n",
    "        controls\n",
    "    ])\n",
    "\n",
    "# Create and display the dashboard\n",
    "dashboard = create_dashboard()\n",
    "display(dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib-based real-time plotting\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('GPU Monitoring Dashboard', fontsize=16)\n",
    "\n",
    "def animate(i):\n",
    "    \"\"\"Animation function for real-time plotting\"\"\"\n",
    "    if not monitor.timestamps:\n",
    "        return\n",
    "    \n",
    "    # Convert timestamps to relative time\n",
    "    times = [t - monitor.timestamps[0] for t in monitor.timestamps]\n",
    "    \n",
    "    # Clear axes\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    ax3.clear()\n",
    "    ax4.clear()\n",
    "    \n",
    "    # Memory usage over time\n",
    "    ax1.plot(times, monitor.memory_used, 'b-', linewidth=2)\n",
    "    ax1.set_title('GPU Memory Usage (MB)')\n",
    "    ax1.set_ylabel('Memory (MB)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Memory percentage\n",
    "    ax2.plot(times, monitor.memory_percent, 'r-', linewidth=2)\n",
    "    ax2.set_title('GPU Memory Usage (%)')\n",
    "    ax2.set_ylabel('Usage (%)')\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # GPU utilization\n",
    "    ax3.plot(times, monitor.gpu_utilization, 'g-', linewidth=2)\n",
    "    ax3.set_title('GPU Utilization (%)')\n",
    "    ax3.set_xlabel('Time (seconds)')\n",
    "    ax3.set_ylabel('Utilization (%)')\n",
    "    ax3.set_ylim(0, 100)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Temperature (if available)\n",
    "    if monitor.temperatures and len(monitor.temperatures) == len(times):\n",
    "        ax4.plot(times, monitor.temperatures, 'orange', linewidth=2)\n",
    "        ax4.set_title('GPU Temperature (°C)')\n",
    "        ax4.set_xlabel('Time (seconds)')\n",
    "        ax4.set_ylabel('Temperature (°C)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Temperature\\ndata not available', \n",
    "                transform=ax4.transAxes, ha='center', va='center')\n",
    "        ax4.set_title('GPU Temperature (°C)')\n",
    "\n",
    "# Note: Animation would run continuously - use with caution in notebooks\n",
    "# To use: ani = animation.FuncAnimation(fig, animate, interval=1000)\n",
    "# plt.show()\n",
    "\n",
    "print(\"Run the following code to start real-time plotting:\")\n",
    "print(\"ani = animation.FuncAnimation(fig, animate, interval=1000)\")\n",
    "print(\"plt.show()\")\n",
    "print(\"\\n(First start monitoring with the widget dashboard above)\")\n",
    "\n",
    "# Display current static plot (run animate once to populate)\n",
    "animate(0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple GPU allocation test for demonstration\n",
    "\n",
    "def test_gpu_allocation():\n",
    "    \"\"\"Allocate some GPU memory to see the monitoring in action\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available - cannot test GPU allocation\")\n",
    "        return\n",
    "    \n",
    "    print(\"Allocating GPU tensors to demonstrate monitoring...\")\n",
    "    \n",
    "    # Allocate increasing amounts of memory\n",
    "    tensors = []\n",
    "    sizes = [100, 500, 1000, 2000]  # MB approximately\n",
    "    \n",
    "    for size in sizes:\n",
    "        # Create tensor that uses roughly 'size' MB\n",
    "        tensor_size = int(size * 1024 * 1024 / 4)  # float32 = 4 bytes\n",
    "        tensor = torch.randn(tensor_size, device='cuda')\n",
    "        tensors.append(tensor)\n",
    "        \n",
    "        # Show current memory usage\n",
    "        info = get_gpu_info()\n",
    "        if 'error' not in info:\n",
    "            print(f\"Allocated ~{size}MB: Memory usage {info['memory_used']:.1f}MB ({info['memory_percent']:.1f}%)\")\n",
    "        \n",
    "        time.sleep(0.5)  # Brief pause to see the change\n",
    "    \n",
    "    print(\"Clearing tensors...\")\n",
    "    del tensors\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    info = get_gpu_info()\n",
    "    if 'error' not in info:\n",
    "        print(f\"After cleanup: Memory usage {info['memory_used']:.1f}MB ({info['memory_percent']:.1f}%)\")\n",
    "\n",
    "# Uncomment to test:\n",
    "# test_gpu_allocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "monitor.stop_monitoring()\n",
    "pynvml.nvmlShutdown()\n",
    "\n",
    "print(\"GPU monitoring cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
